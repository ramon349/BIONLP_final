{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys\\n!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\\n#'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "st = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def loadDataAsDataFrame(f_path):\n",
    "    '''\n",
    "        Given a path, loads a data set and puts it into a dataframe\n",
    "        - simplified mechanism\n",
    "    '''\n",
    "    df = pd.read_csv(f_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "\n",
    "    # Replace/remove username\n",
    "    raw_text = re.sub('(@[A-Za-z0-9\\_]+)', '@username_', raw_text)\n",
    "    #stemming and lowercasing\n",
    "    words=[]\n",
    "    for w in raw_text.lower().split():\n",
    "        if not w in st and not w in ['.',',', '[', ']', '(', ')']:\n",
    "            words.append(w)\n",
    "            \n",
    "    return (\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1143</td>\n",
       "      <td>835999046204551168</td>\n",
       "      <td>2017-02-26 23:44:39</td>\n",
       "      <td>my mom could have worked while dying from stag...</td>\n",
       "      <td>0</td>\n",
       "      <td>mom could worked dying stage 4 breast cancer &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1981</td>\n",
       "      <td>839261636275933184</td>\n",
       "      <td>2017-03-07 23:49:01</td>\n",
       "      <td>new drug 4 breast cancer is $10k per month. Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>new drug 4 breast cancer $10k per month. affor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1153</td>\n",
       "      <td>853032936324464640</td>\n",
       "      <td>2017-04-14 23:51:15</td>\n",
       "      <td>When people who don't know me try to educate m...</td>\n",
       "      <td>1</td>\n",
       "      <td>people know try educate breast cancer patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1154</td>\n",
       "      <td>859919598904254464</td>\n",
       "      <td>2017-05-03 23:56:23</td>\n",
       "      <td>This 11 year breast cancer survivor needs heal...</td>\n",
       "      <td>0</td>\n",
       "      <td>11 year breast cancer survivor needs healthcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1159</td>\n",
       "      <td>861368530817732608</td>\n",
       "      <td>2017-05-07 23:53:55</td>\n",
       "      <td>@KellyMazeski Fellow breast cancer survivor he...</td>\n",
       "      <td>0</td>\n",
       "      <td>@username_ fellow breast cancer survivor donat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>5991</td>\n",
       "      <td>5991</td>\n",
       "      <td>1235419985733918976</td>\n",
       "      <td>2020-03-05 04:20:52</td>\n",
       "      <td>b'just got back to bk and found out my aunt ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'just got back bk found aunt breast cancer da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>5992</td>\n",
       "      <td>5992</td>\n",
       "      <td>1224811121976271104</td>\n",
       "      <td>2020-02-04 21:45:02</td>\n",
       "      <td>b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>5993</td>\n",
       "      <td>5993</td>\n",
       "      <td>1234769787672302080</td>\n",
       "      <td>2020-03-03 09:17:12</td>\n",
       "      <td>b'@dailymailceleb @dailymailuk lovely see @kyl...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'@username_ @username_ lovely see @username_ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>5994</td>\n",
       "      <td>5994</td>\n",
       "      <td>1235541633812185088</td>\n",
       "      <td>2020-03-05 12:24:15</td>\n",
       "      <td>b'@drkcain as a breast cancer patient myself f...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'@username_ breast cancer patient 27 yrs, two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5995</td>\n",
       "      <td>5995</td>\n",
       "      <td>1225497981379579904</td>\n",
       "      <td>2020-02-06 19:14:22</td>\n",
       "      <td>b'@aasfoundation1 @claranlee @univsurg @gdkenn...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'@username_ @username_ @username_ @username_ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5019 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1                   ID            Timestamp  \\\n",
       "0              0          1143   835999046204551168  2017-02-26 23:44:39   \n",
       "1              2          1981   839261636275933184  2017-03-07 23:49:01   \n",
       "2              5          1153   853032936324464640  2017-04-14 23:51:15   \n",
       "3              7          1154   859919598904254464  2017-05-03 23:56:23   \n",
       "4              9          1159   861368530817732608  2017-05-07 23:53:55   \n",
       "...          ...           ...                  ...                  ...   \n",
       "5014        5991          5991  1235419985733918976  2020-03-05 04:20:52   \n",
       "5015        5992          5992  1224811121976271104  2020-02-04 21:45:02   \n",
       "5016        5993          5993  1234769787672302080  2020-03-03 09:17:12   \n",
       "5017        5994          5994  1235541633812185088  2020-03-05 12:24:15   \n",
       "5018        5995          5995  1225497981379579904  2020-02-06 19:14:22   \n",
       "\n",
       "                                                   Text  Class  \\\n",
       "0     my mom could have worked while dying from stag...      0   \n",
       "1     new drug 4 breast cancer is $10k per month. Ca...      0   \n",
       "2     When people who don't know me try to educate m...      1   \n",
       "3     This 11 year breast cancer survivor needs heal...      0   \n",
       "4     @KellyMazeski Fellow breast cancer survivor he...      0   \n",
       "...                                                 ...    ...   \n",
       "5014  b'just got back to bk and found out my aunt ha...      0   \n",
       "5015  b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...      1   \n",
       "5016  b'@dailymailceleb @dailymailuk lovely see @kyl...      0   \n",
       "5017  b'@drkcain as a breast cancer patient myself f...      1   \n",
       "5018  b'@aasfoundation1 @claranlee @univsurg @gdkenn...      0   \n",
       "\n",
       "                                     preprocessed_texts  \n",
       "0     mom could worked dying stage 4 breast cancer &...  \n",
       "1     new drug 4 breast cancer $10k per month. affor...  \n",
       "2     people know try educate breast cancer patient ...  \n",
       "3     11 year breast cancer survivor needs healthcar...  \n",
       "4     @username_ fellow breast cancer survivor donat...  \n",
       "...                                                 ...  \n",
       "5014  b'just got back bk found aunt breast cancer da...  \n",
       "5015  b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...  \n",
       "5016  b'@username_ @username_ lovely see @username_ ...  \n",
       "5017  b'@username_ breast cancer patient 27 yrs, two...  \n",
       "5018  b'@username_ @username_ @username_ @username_ ...  \n",
       "\n",
       "[5019 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "f_path = './Breast Cancer(Raw_data_2_Classes).csv'\n",
    "data = loadDataAsDataFrame(f_path)\n",
    "\n",
    "texts = data['Text']\n",
    "classes = data['Class']\n",
    "ids = data['ID']\n",
    "\n",
    "#PREPROCESS THE DATA\n",
    "texts_preprocessed=[preprocess_text(txt) for txt in texts]\n",
    "data['preprocessed_texts']=texts_preprocessed\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This 11 year breast cancer survivor needs healthcare!  Medicare for all!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3736, 1: 1283})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_clusters = {}\n",
    "\n",
    "def loadwordclusters():\n",
    "    infile = open('./50mpaths2',  \"r\", encoding=\"utf-8\")\n",
    "    for line in infile:\n",
    "        items = str.strip(line).split()\n",
    "        class_ = items[0]\n",
    "        term = items[1]\n",
    "        word_clusters[term] = class_\n",
    "    return word_clusters\n",
    "\n",
    "def getclusterfeatures(sent):\n",
    "    sent = sent.lower()\n",
    "    terms = nltk.word_tokenize(sent)\n",
    "    cluster_string = ''\n",
    "    for t in terms:\n",
    "        if t in word_clusters.keys():\n",
    "                cluster_string += 'clust_' + word_clusters[t] + '_clust '\n",
    "    return str.strip(cluster_string)\n",
    "\n",
    "loadwordclusters()\n",
    "\n",
    "class myVectorizer():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):\n",
    "        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='f1_macro',n_jobs=-1)\n",
    "        grid_search.fit(training_texts, training_classes)\n",
    "        return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set_size = int(0.8*len(data))\n",
    "\n",
    "X=data\n",
    "y=data['Class'].tolist()\n",
    "\n",
    "training_rows, test_rows, training_classes, test_classes = train_test_split(\n",
    "    X, y, train_size=training_set_size, random_state=42069)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(value, maxOfList, minOfList):\n",
    "    return (value - minOfList) / (maxOfList - minOfList)\n",
    "    \n",
    "def getNormalizedList(values, maxOfList, minOfList):\n",
    "    ret = []\n",
    "    for value in values:\n",
    "        ret.append(normalize(value, maxOfList, minOfList))\n",
    "        \n",
    "    return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf1\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def bulkEval(predictions_test, test_classes, bs=False):\n",
    "    print (\"Accuracy\\t\", acc(predictions_test,test_classes))\n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(test_classes, predictions_test, labels=[1,0], normalize='true'))\n",
    "\n",
    "    #bootstrap it\n",
    "    if bs:\n",
    "        f1s=[]\n",
    "        for iteration in range(1000):\n",
    "            resampleIndexes=random.choices(range(len(predictions_test)), k=1000)\n",
    "            resamplePreds=[predictions_test[i] for i in resampleIndexes]\n",
    "            resampleTrueClasses=[test_classes[i] for i in resampleIndexes]\n",
    "            thisF1=f1(resamplePreds,resampleTrueClasses, average='macro')\n",
    "            f1s.append(thisF1)\n",
    "\n",
    "        print(\"Bootstrapping 95% confidence interval:\")\n",
    "        interval=np.percentile(f1s, [2.5, 97.5])\n",
    "        print(interval)\n",
    "\n",
    "    print(\"\\t****************************************\\n\")\n",
    "\n",
    "    #entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "    #          \"Confidence Interval\":interval}\n",
    "    #f1df=f1df.append(entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess 0 Classifier\n",
    "The Stupidest classifier possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stupidClassifier():\n",
    "    def fit(self, X,y):\n",
    "        doNothing=True\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ret = []\n",
    "        for xi in X.iterrows():\n",
    "            ret.append(0)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.7609561752988048\n",
      "F1 Macro\t 0.4321266968325792\n",
      "F1 Micro\t 0.7609561752988048\n",
      "Confusion Matrix\n",
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "Bootstrapping 95% confidence interval:\n",
      "[0.42329873 0.44040291]\n",
      "\t****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFIER\n",
    "clf=stupidClassifier()\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = clf.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNB baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.7081673306772909\n",
      "F1 Macro\t 0.6134437659574747\n",
      "F1 Micro\t 0.7081673306772909\n",
      "Confusion Matrix\n",
      "[[0.44583333 0.55416667]\n",
      " [0.20942408 0.79057592]]\n",
      "Bootstrapping 95% confidence interval:\n",
      "[0.58014509 0.64687017]\n",
      "\t****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "#CLASSIFIER\n",
    "gnb_classifier = GaussianNB()\n",
    "grid_params = {}\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',gnb_classifier)])\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 5\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple transformer RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Garry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Garry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Garry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Garry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507e05a6d54c4f95b919fe2519761051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4015.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4b8c63973448e6804d6a873229013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e307e3f4c0bd429d9f77761ecc67104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 1'), FloatProgress(value=0.0, max=502.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garry\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\optim\\lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garry\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\optim\\lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93591fee361d48ebaf660c7999b3898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1004.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5978a0d4d8af41bba644cd2cc719165c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=126.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1 score\n",
      "0.8312757201646092\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "model_args={'overwrite_output_dir':True}\n",
    "\n",
    "# Create a TransformerModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', use_cuda=False, args=model_args)\n",
    "#model = ClassificationModel('roberta', 'roberta-base', use_cuda=True, args=model_args)\n",
    "\n",
    "#change our data into a format that simpletransformers can process\n",
    "training_rows['text']=training_rows['Text']\n",
    "training_rows['labels']=training_rows['Class']\n",
    "test_rows['text']=test_rows['Text']\n",
    "test_rows['labels']=test_rows['Class']\n",
    "\n",
    "# Train the model\n",
    "model.train_model(training_rows)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_rows)\n",
    "\n",
    "print(\"f1 score\")\n",
    "precision=result['tp'] / (result['tp'] + result['fp'])\n",
    "recall=result['tp'] / (result['tp'] + result['fn'])\n",
    "f1score= 2 * precision * recall / (precision + recall)\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "\n",
    "Best hyperparameters:\n",
    "{'svm_classifier__C': 4, 'svm_classifier__kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "#CLASSIFIER\n",
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [0.25,1,4,16,64],\n",
    "     'svm_classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [4],\n",
    "     'svm_classifier__kernel': ['rbf'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(bulkEval(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_estimators': 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',rf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_estimators': np.arange(5,60,5)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "print('Optimal n found:', grid.best_params_['classifier__n_estimators'])\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_neighbors': 3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= KNeighborsClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_neighbors': np.arange(1,20,1),\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Is it really data science if there isn't a neural network somewhere?\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__hidden_layer_sizes': (11,)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= MLPClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     'classifier__hidden_layer_sizes': layerParams,\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaboost\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__base_estimator__max_depth': 3, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(), random_state=420)\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     \"classifier__base_estimator__n_estimators\":range(10, 51, 20),\n",
    "     \"classifier__base_estimator__max_depth\":range(3),\n",
    "    \"classifier__n_estimators\":range(10, 51, 20)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now evaluate them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Confusion matrix whose i-th row and j-th column entry indicates the number of samples \n",
    "with true label being i-th class and prediced label being j-th class.\n",
    "\"\"\"\n",
    "\n",
    "gnb = GaussianNB()\n",
    "svmc = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "nn=MLPClassifier(hidden_layer_sizes=(11,))\n",
    "en=VotingClassifier(estimators=[('SVM', svmc), ('RF', rf), \n",
    "                                (\"KNN\", knn), (\"NN\", nn)], \n",
    "                                      voting='soft')\n",
    "\n",
    "f1df=pd.DataFrame()\n",
    "\n",
    "for clf in [gnb, svmc, rf, knn, nn, en]:\n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 2\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    print('Best hyperparameters:')\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    print(\"Classifier\\t\", clf)\n",
    "\n",
    "    print (\"Accuracy\\t\", acc(predictions_test,test_classes))\n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(test_classes, predictions_test, labels=['CoM', 'Other'], normalize='true'))\n",
    "    \n",
    "    #bootstrap it\n",
    "    f1s=[]\n",
    "    for iteration in range(1000):\n",
    "        resampleIndexes=random.choices(range(len(predictions_test)), k=len(predictions_test))\n",
    "        resamplePreds=[predictions_test[i] for i in resampleIndexes]\n",
    "        resampleTrueClasses=[test_classes[i] for i in resampleIndexes]\n",
    "        thisF1=f1(resamplePreds,resampleTrueClasses, average='macro')\n",
    "        f1s.append(thisF1)\n",
    "        \n",
    "    print(\"Bootstrapping 95% confidence interval:\")\n",
    "    interval=np.percentile(f1s, [2.5, 97.5])\n",
    "    print(interval)\n",
    "    \n",
    "    print(\"\\t****************************************\\n\")\n",
    "    \n",
    "    entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"Confidence Interval\":interval}\n",
    "    f1df=f1df.append(entry, ignore_index=True)\n",
    "    \n",
    "f1df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The winner: Adaboost RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICKLE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "vectorizer=myVectorizer()\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(data, data['Class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = pipeline.predict(test_rows)\n",
    "bulkEval(predictions_test, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'adaForest10_22_2020.pickle'\n",
    "#pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just give me a function that takes an input and spits out a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'adaForest10_22_2020.pickle'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def textToPred(txt):\n",
    "    inputDF=pd.DataFrame({'Text':[txt]})\n",
    "    pred=model.predict(inputDF)[0]\n",
    "    return pred\n",
    "\n",
    "textToPred(\"My micro pen is not a self-report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove fall duration\n",
    "class ablation1():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "            \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove metamap tags of locations\n",
    "class ablation2():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove metamap tags of fall descriptions\n",
    "class ablation3():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove 50mpaths clusters of fall descriptions\n",
    "class ablation4():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "#remove the n-grams\n",
    "class ablation5:\n",
    "    def __init__(self):\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "abDF=pd.DataFrame()\n",
    "\n",
    "for vectorizer in [ablation1(), ablation2(), ablation3(), ablation4(), ablation5()]:\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 5\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"\\t****************************************\\n\")\n",
    "    \n",
    "    entry={\"F1 Macro\":macro, \"F1 Micro\":micro}\n",
    "    abDF=abDF.append(entry, ignore_index=True)\n",
    "    \n",
    "abDF['Features Removed']=['Fall Duration', 'Metamap Location Tags', 'Metamap Description Tags', \n",
    "                          'TweetNLP Description Tags', 'N-Grams']\n",
    "abDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training size vs performance (F1 macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for frac in np.arange(.4, 1.01, .02):\n",
    "    \n",
    "    partial_training_set_size=int(frac*training_set_size)\n",
    "    partial_training_rows = training_rows.sample(n=partial_training_set_size)\n",
    "    partial_training_classes=partial_training_rows['target'].tolist()\n",
    "    \n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 5\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, partial_training_rows,partial_training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    x.append(partial_training_set_size)\n",
    "    y.append(f1(predictions_test,test_classes, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kite.com/python/answers/how-to-plot-a-linear-regression-line-on-a-scatter-plot-in-python\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.plot(x, [m*xi + b for xi in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
