{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys\\n!{sys.executable} -m pip install transformers\\n#'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "st = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def loadDataAsDataFrame(f_path):\n",
    "    '''\n",
    "        Given a path, loads a data set and puts it into a dataframe\n",
    "        - simplified mechanism\n",
    "    '''\n",
    "    df = pd.read_csv(f_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "\n",
    "    # Replace/remove username\n",
    "    raw_text = re.sub('(@[A-Za-z0-9\\_]+)', '@username_', raw_text)\n",
    "    #stemming and lowercasing\n",
    "    words=[]\n",
    "    for w in raw_text.lower().split():\n",
    "        if not w in st and not w in ['.',',', '[', ']', '(', ')']:\n",
    "            words.append(w)\n",
    "            \n",
    "    return (\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1143</td>\n",
       "      <td>835999046204551168</td>\n",
       "      <td>2017-02-26 23:44:39</td>\n",
       "      <td>my mom could have worked while dying from stag...</td>\n",
       "      <td>0</td>\n",
       "      <td>mom could worked dying stage 4 breast cancer &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1981</td>\n",
       "      <td>839261636275933184</td>\n",
       "      <td>2017-03-07 23:49:01</td>\n",
       "      <td>new drug 4 breast cancer is $10k per month. Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>new drug 4 breast cancer $10k per month. affor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1153</td>\n",
       "      <td>853032936324464640</td>\n",
       "      <td>2017-04-14 23:51:15</td>\n",
       "      <td>When people who don't know me try to educate m...</td>\n",
       "      <td>1</td>\n",
       "      <td>people know try educate breast cancer patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1154</td>\n",
       "      <td>859919598904254464</td>\n",
       "      <td>2017-05-03 23:56:23</td>\n",
       "      <td>This 11 year breast cancer survivor needs heal...</td>\n",
       "      <td>0</td>\n",
       "      <td>11 year breast cancer survivor needs healthcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1159</td>\n",
       "      <td>861368530817732608</td>\n",
       "      <td>2017-05-07 23:53:55</td>\n",
       "      <td>@KellyMazeski Fellow breast cancer survivor he...</td>\n",
       "      <td>0</td>\n",
       "      <td>@username_ fellow breast cancer survivor donat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>5991</td>\n",
       "      <td>5991</td>\n",
       "      <td>1235419985733918976</td>\n",
       "      <td>2020-03-05 04:20:52</td>\n",
       "      <td>b'just got back to bk and found out my aunt ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'just got back bk found aunt breast cancer da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>5992</td>\n",
       "      <td>5992</td>\n",
       "      <td>1224811121976271104</td>\n",
       "      <td>2020-02-04 21:45:02</td>\n",
       "      <td>b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>5993</td>\n",
       "      <td>5993</td>\n",
       "      <td>1234769787672302080</td>\n",
       "      <td>2020-03-03 09:17:12</td>\n",
       "      <td>b'@dailymailceleb @dailymailuk lovely see @kyl...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'@username_ @username_ lovely see @username_ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>5994</td>\n",
       "      <td>5994</td>\n",
       "      <td>1235541633812185088</td>\n",
       "      <td>2020-03-05 12:24:15</td>\n",
       "      <td>b'@drkcain as a breast cancer patient myself f...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'@username_ breast cancer patient 27 yrs, two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5995</td>\n",
       "      <td>5995</td>\n",
       "      <td>1225497981379579904</td>\n",
       "      <td>2020-02-06 19:14:22</td>\n",
       "      <td>b'@aasfoundation1 @claranlee @univsurg @gdkenn...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'@username_ @username_ @username_ @username_ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5019 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1                   ID            Timestamp  \\\n",
       "0              0          1143   835999046204551168  2017-02-26 23:44:39   \n",
       "1              2          1981   839261636275933184  2017-03-07 23:49:01   \n",
       "2              5          1153   853032936324464640  2017-04-14 23:51:15   \n",
       "3              7          1154   859919598904254464  2017-05-03 23:56:23   \n",
       "4              9          1159   861368530817732608  2017-05-07 23:53:55   \n",
       "...          ...           ...                  ...                  ...   \n",
       "5014        5991          5991  1235419985733918976  2020-03-05 04:20:52   \n",
       "5015        5992          5992  1224811121976271104  2020-02-04 21:45:02   \n",
       "5016        5993          5993  1234769787672302080  2020-03-03 09:17:12   \n",
       "5017        5994          5994  1235541633812185088  2020-03-05 12:24:15   \n",
       "5018        5995          5995  1225497981379579904  2020-02-06 19:14:22   \n",
       "\n",
       "                                                   Text  Class  \\\n",
       "0     my mom could have worked while dying from stag...      0   \n",
       "1     new drug 4 breast cancer is $10k per month. Ca...      0   \n",
       "2     When people who don't know me try to educate m...      1   \n",
       "3     This 11 year breast cancer survivor needs heal...      0   \n",
       "4     @KellyMazeski Fellow breast cancer survivor he...      0   \n",
       "...                                                 ...    ...   \n",
       "5014  b'just got back to bk and found out my aunt ha...      0   \n",
       "5015  b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...      1   \n",
       "5016  b'@dailymailceleb @dailymailuk lovely see @kyl...      0   \n",
       "5017  b'@drkcain as a breast cancer patient myself f...      1   \n",
       "5018  b'@aasfoundation1 @claranlee @univsurg @gdkenn...      0   \n",
       "\n",
       "                                     preprocessed_texts  \n",
       "0     mom could worked dying stage 4 breast cancer &...  \n",
       "1     new drug 4 breast cancer $10k per month. affor...  \n",
       "2     people know try educate breast cancer patient ...  \n",
       "3     11 year breast cancer survivor needs healthcar...  \n",
       "4     @username_ fellow breast cancer survivor donat...  \n",
       "...                                                 ...  \n",
       "5014  b'just got back bk found aunt breast cancer da...  \n",
       "5015  b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...  \n",
       "5016  b'@username_ @username_ lovely see @username_ ...  \n",
       "5017  b'@username_ breast cancer patient 27 yrs, two...  \n",
       "5018  b'@username_ @username_ @username_ @username_ ...  \n",
       "\n",
       "[5019 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "f_path = './Breast Cancer(Raw_data_2_Classes).csv'\n",
    "data = loadDataAsDataFrame(f_path)\n",
    "\n",
    "texts = data['Text']\n",
    "classes = data['Class']\n",
    "ids = data['ID']\n",
    "\n",
    "#PREPROCESS THE DATA\n",
    "texts_preprocessed=[preprocess_text(txt) for txt in texts]\n",
    "data['preprocessed_texts']=texts_preprocessed\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This 11 year breast cancer survivor needs healthcare!  Medicare for all!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3736, 1: 1283})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_clusters = {}\n",
    "\n",
    "def loadwordclusters():\n",
    "    infile = open('./50mpaths2',  \"r\", encoding=\"utf-8\")\n",
    "    for line in infile:\n",
    "        items = str.strip(line).split()\n",
    "        class_ = items[0]\n",
    "        term = items[1]\n",
    "        word_clusters[term] = class_\n",
    "    return word_clusters\n",
    "\n",
    "def getclusterfeatures(sent):\n",
    "    sent = sent.lower()\n",
    "    terms = nltk.word_tokenize(sent)\n",
    "    cluster_string = ''\n",
    "    for t in terms:\n",
    "        if t in word_clusters.keys():\n",
    "                cluster_string += 'clust_' + word_clusters[t] + '_clust '\n",
    "    return str.strip(cluster_string)\n",
    "\n",
    "loadwordclusters()\n",
    "\n",
    "class myVectorizer():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):\n",
    "        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='f1_macro',n_jobs=-1)\n",
    "        grid_search.fit(training_texts, training_classes)\n",
    "        return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set_size = int(0.8*len(data))\n",
    "\n",
    "X=data\n",
    "y=data['Class'].tolist()\n",
    "\n",
    "training_rows, test_rows, training_classes, test_classes = train_test_split(\n",
    "    X, y, train_size=training_set_size, random_state=42069)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(value, maxOfList, minOfList):\n",
    "    return (value - minOfList) / (maxOfList - minOfList)\n",
    "    \n",
    "def getNormalizedList(values, maxOfList, minOfList):\n",
    "    ret = []\n",
    "    for value in values:\n",
    "        ret.append(normalize(value, maxOfList, minOfList))\n",
    "        \n",
    "    return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf1\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def bulkEval(predictions_test, test_classes, bs=False):\n",
    "    print (\"Accuracy\\t\", acc(predictions_test,test_classes))\n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(test_classes, predictions_test, labels=[1,0], normalize='true'))\n",
    "\n",
    "    #bootstrap it\n",
    "    if bs:\n",
    "        f1s=[]\n",
    "        for iteration in range(1000):\n",
    "            resampleIndexes=random.choices(range(len(predictions_test)), k=1000)\n",
    "            resamplePreds=[predictions_test[i] for i in resampleIndexes]\n",
    "            resampleTrueClasses=[test_classes[i] for i in resampleIndexes]\n",
    "            thisF1=f1(resamplePreds,resampleTrueClasses, average='macro')\n",
    "            f1s.append(thisF1)\n",
    "\n",
    "        print(\"Bootstrapping 95% confidence interval:\")\n",
    "        interval=np.percentile(f1s, [2.5, 97.5])\n",
    "        print(interval)\n",
    "\n",
    "    print(\"\\t****************************************\\n\")\n",
    "\n",
    "    #entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "    #          \"Confidence Interval\":interval}\n",
    "    #f1df=f1df.append(entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess 0 Classifier\n",
    "The Stupidest classifier possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stupidClassifier():\n",
    "    def fit(self, X,y):\n",
    "        doNothing=True\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ret = []\n",
    "        for xi in X.iterrows():\n",
    "            ret.append(0)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.7609561752988048\n",
      "F1 Macro\t 0.4321266968325792\n",
      "F1 Micro\t 0.7609561752988048\n",
      "Confusion Matrix\n",
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "Bootstrapping 95% confidence interval:\n",
      "[0.42394652 0.44071588]\n",
      "\t****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFIER\n",
    "clf=stupidClassifier()\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = clf.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNB baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.7081673306772909\n",
      "F1 Macro\t 0.6134437659574747\n",
      "F1 Micro\t 0.7081673306772909\n",
      "Confusion Matrix\n",
      "[[0.44583333 0.55416667]\n",
      " [0.20942408 0.79057592]]\n",
      "Bootstrapping 95% confidence interval:\n",
      "[0.58135878 0.64601528]\n",
      "\t****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "#CLASSIFIER\n",
    "gnb_classifier = GaussianNB()\n",
    "grid_params = {}\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',gnb_classifier)])\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 5\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "\n",
    "Best hyperparameters:\n",
    "{'svm_classifier__C': 4, 'svm_classifier__kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'svm_classifier__C': 4, 'svm_classifier__kernel': 'rbf'}\n",
      "Performance on held-out test set ... :\n",
      "0.8545816733067729\n"
     ]
    }
   ],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "#CLASSIFIER\n",
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [0.25,1,4,16,64],\n",
    "     'svm_classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.8545816733067729\n",
      "F1 Macro\t 0.7935145795182421\n",
      "F1 Micro\t 0.854581673306773\n",
      "Confusion Matrix\n",
      "[[0.65       0.35      ]\n",
      " [0.08115183 0.91884817]]\n",
      "\t****************************************\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [4],\n",
    "     'svm_classifier__kernel': ['rbf'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(bulkEval(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_estimators': 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__n_estimators': 5}\n",
      "Optimal n found: 5\n",
      "Performance on held-out test set ... :\n",
      "0.7918326693227091\n"
     ]
    }
   ],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',rf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_estimators': np.arange(5,60,5)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "print('Optimal n found:', grid.best_params_['classifier__n_estimators'])\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_neighbors': 3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__n_neighbors': 3}\n",
      "Performance on held-out test set ... :\n",
      "0.7539840637450199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= KNeighborsClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_neighbors': np.arange(1,20,1),\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Is it really data science if there isn't a neural network somewhere?\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__hidden_layer_sizes': (11,)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__hidden_layer_sizes': (11,)}\n",
      "Performance on held-out test set ... :\n",
      "0.8376494023904383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= MLPClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     'classifier__hidden_layer_sizes': layerParams,\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaboost\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__base_estimator__max_depth': 3, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__base_estimator__max_depth': 3, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}\n",
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.8575697211155379\n",
      "F1 Macro\t 0.77551687313448\n",
      "F1 Micro\t 0.8575697211155379\n",
      "Confusion Matrix\n",
      "[[0.52916667 0.47083333]\n",
      " [0.03926702 0.96073298]]\n",
      "\t****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(), random_state=420)\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     \"classifier__base_estimator__n_estimators\":range(10, 51, 20),\n",
    "     \"classifier__base_estimator__max_depth\":range(3),\n",
    "    \"classifier__n_estimators\":range(10, 51, 20)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "That combines all of the classifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "svmc = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "nn=MLPClassifier(hidden_layer_sizes=(11,))\n",
    "\n",
    "en=VotingClassifier(estimators=[('SVM', svmc), ('RF', rf), \n",
    "                                (\"KNN\", knn), (\"NN\", nn)])\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',en)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "    \n",
    "grid_params = {\n",
    "     'classifier__voting': ['soft', 'hard']\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 5\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Garry/.cache\\torch\\hub\\pytorch_fairseq_master\n",
      "2020-10-18 19:19:44 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at C:\\Users\\Garry\\.cache\\torch\\pytorch_fairseq\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "2020-10-18 19:19:47 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 31414,   232,   328,     2])\n",
      "tensor([[[-0.0888, -0.1186, -0.0443,  ..., -0.0934,  0.0559,  0.1610],\n",
      "         [ 0.1948,  0.2413, -0.6944,  ..., -0.0831, -0.4706, -0.0385],\n",
      "         [ 0.0776, -0.0656, -0.1613,  ...,  0.2186, -0.2322,  0.2784],\n",
      "         [-0.0315, -0.4819, -0.3349,  ...,  0.1708, -0.2059,  0.0397],\n",
      "         [-0.0407, -0.1071, -0.0160,  ..., -0.1310, -0.0015,  0.1044]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokens = roberta.encode('Hello world!')\n",
    "print(tokens)\n",
    "assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "assert roberta.decode(tokens) == 'Hello world!'\n",
    "\n",
    "# Extract the last layer's features\n",
    "last_layer_features = roberta.extract_features(tokens)\n",
    "print(last_layer_features)\n",
    "assert last_layer_features.size() == torch.Size([1, 5, 1024])\n",
    "\n",
    "# Extract all layer's features (layer 0 is the embedding layer)\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Garry/.cache\\torch\\hub\\pytorch_fairseq_master\n",
      "2020-10-18 19:23:44 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz from cache at C:\\Users\\Garry\\.cache\\torch\\pytorch_fairseq\\7685ba8546f9a5ce1a00c7a6d7d44f7e748d22681172f0f391c3d48f487c801c.74e37d47306b3cc51c5f8d335022a392c29f1906c8cd9e9cd3446d7422cf55d8\n",
      "2020-10-18 19:23:47 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Download RoBERTa already finetuned for MNLI\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "roberta.eval()  # disable dropout for evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode a pair of sentences and make a prediction\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    print(prediction)\n",
    "    assert prediction == 0  # contradiction\n",
    "\n",
    "    # Encode another pair of sentences\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    print(prediction)\n",
    "    assert prediction == 2  # entailment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 19:26:54 | INFO | filelock | Lock 1733687291848 acquired on C:\\Users\\Garry/.cache\\torch\\transformers\\d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc2a80219544a329a4dd789475d56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 19:26:54 | INFO | filelock | Lock 1733687291848 released on C:\\Users\\Garry/.cache\\torch\\transformers\\d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
      "2020-10-18 19:26:54 | INFO | filelock | Lock 1733687127816 acquired on C:\\Users\\Garry/.cache\\torch\\transformers\\b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be7e7a3bd684b548ebb528370ae6c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 19:26:55 | INFO | filelock | Lock 1733687127816 released on C:\\Users\\Garry/.cache\\torch\\transformers\\b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 20920, 232, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer(\"Hello world\")['input_ids']\n",
    "tokenizer(\" Hello world\")['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my mom could have worked while dying from stage 4 breast cancer & paid for tx out of pocket?', 'new drug 4 breast cancer is $10k per month. Can we ALL afford spending that much on 1  while others have 0', \"When people who don't know me try to educate me on what a breast cancer patient goes through...\", 'This 11 year breast cancer survivor needs healthcare!  Medicare for all!', '@KellyMazeski Fellow breast cancer survivor here just donated to your campaign! Born & raised in IL, now live in NY. Pulling for you!!']\n",
      "{'input_ids': [[0, 4783, 3795, 115, 33, 1006, 150, 8180, 31, 1289, 204, 6181, 1668, 359, 1199, 13, 48726, 66, 9, 7524, 116, 2], [0, 4651, 1262, 204, 6181, 1668, 16, 68, 698, 330, 228, 353, 4, 2615, 52, 12389, 4960, 1408, 14, 203, 15, 112, 1437, 150, 643, 33, 321, 2], [0, 1779, 82, 54, 218, 75, 216, 162, 860, 7, 11427, 162, 15, 99, 10, 6181, 1668, 3186, 1411, 149, 734, 2], [0, 713, 365, 76, 6181, 1668, 14466, 782, 3717, 328, 1437, 8999, 13, 70, 328, 2], [0, 1039, 34313, 448, 24505, 3144, 16728, 6181, 1668, 14466, 259, 95, 6652, 7, 110, 637, 328, 8912, 359, 1179, 11, 11935, 6, 122, 697, 11, 5300, 4, 18143, 154, 13, 47, 12846, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[22, 28, 22, 16, 34]\n"
     ]
    }
   ],
   "source": [
    "tsents=data['Text'].tolist()[:5]\n",
    "print(tsents)\n",
    "print(tokenizer(tsents))\n",
    "print([len(x) for x in tokenizer(tsents)['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now evaluate them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Confusion matrix whose i-th row and j-th column entry indicates the number of samples \n",
    "with true label being i-th class and prediced label being j-th class.\n",
    "\"\"\"\n",
    "\n",
    "gnb = GaussianNB()\n",
    "svmc = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "nn=MLPClassifier(hidden_layer_sizes=(11,))\n",
    "en=VotingClassifier(estimators=[('SVM', svmc), ('RF', rf), \n",
    "                                (\"KNN\", knn), (\"NN\", nn)], \n",
    "                                      voting='soft')\n",
    "\n",
    "f1df=pd.DataFrame()\n",
    "\n",
    "for clf in [gnb, svmc, rf, knn, nn, en]:\n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 2\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    print('Best hyperparameters:')\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    print(\"Classifier\\t\", clf)\n",
    "\n",
    "    print (\"Accuracy\\t\", acc(predictions_test,test_classes))\n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(test_classes, predictions_test, labels=['CoM', 'Other'], normalize='true'))\n",
    "    \n",
    "    #bootstrap it\n",
    "    f1s=[]\n",
    "    for iteration in range(1000):\n",
    "        resampleIndexes=random.choices(range(len(predictions_test)), k=len(predictions_test))\n",
    "        resamplePreds=[predictions_test[i] for i in resampleIndexes]\n",
    "        resampleTrueClasses=[test_classes[i] for i in resampleIndexes]\n",
    "        thisF1=f1(resamplePreds,resampleTrueClasses, average='macro')\n",
    "        f1s.append(thisF1)\n",
    "        \n",
    "    print(\"Bootstrapping 95% confidence interval:\")\n",
    "    interval=np.percentile(f1s, [2.5, 97.5])\n",
    "    print(interval)\n",
    "    \n",
    "    print(\"\\t****************************************\\n\")\n",
    "    \n",
    "    entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"Confidence Interval\":interval}\n",
    "    f1df=f1df.append(entry, ignore_index=True)\n",
    "    \n",
    "f1df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The winner: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove fall duration\n",
    "class ablation1():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "            \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove metamap tags of locations\n",
    "class ablation2():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove metamap tags of fall descriptions\n",
    "class ablation3():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "\n",
    "#remove 50mpaths clusters of fall descriptions\n",
    "class ablation4():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        texts_preprocessed = []\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "#remove the n-grams\n",
    "class ablation5:\n",
    "    def __init__(self):\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.cnumclustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        self.loccnumClusterVectorizer= CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        \n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        allcnums=[]\n",
    "        allCNumLists=rows['CNums']\n",
    "        for cnumList in allCNumLists:\n",
    "            allcnums.append(cnumList)\n",
    "        \n",
    "        self.clustervectorizer.fit(clusters)\n",
    "        self.cnumclustervectorizer.fit(allcnums)  \n",
    "        \n",
    "        #fall location\n",
    "        allLoccnums=[]\n",
    "        allLocCNumLists=rows['LocCNums']\n",
    "        for cnumList in allLocCNumLists:\n",
    "            allLoccnums.append(cnumList)\n",
    "            \n",
    "        self.loccnumClusterVectorizer.fit(allLoccnums)  \n",
    "        \n",
    "        \n",
    "        #get ready to normalize all the other features\n",
    "        for feature in training_rows.columns:\n",
    "            values=training_rows[feature]\n",
    "            featureType=type(values[0])\n",
    "\n",
    "            if not featureType==str or featureType==int:\n",
    "                self.maxs[feature]=max(values)\n",
    "                self.mins[feature]=min(values)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['fall_description']\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        \n",
    "        allcnums=rows['CNums']\n",
    "        cnum_cluster_vectors = self.cnumclustervectorizer.transform(allcnums).toarray()\n",
    "        allloccnums=rows['LocCNums']\n",
    "        loc_cnum_cluster_vectors = self.loccnumClusterVectorizer.transform(allloccnums).toarray()\n",
    "        \n",
    "        data_vectors = np.concatenate((data_vectors, cnum_cluster_vectors), axis=1)\n",
    "        data_vectors = np.concatenate((data_vectors, loc_cnum_cluster_vectors), axis=1)\n",
    "\n",
    "        \n",
    "        #tack on all the other numeric features\n",
    "        for feature in ['duration',]:\n",
    "            values=rows[feature]\n",
    "            normValues = np.array([getNormalizedList(values, self.maxs[feature], self.mins[feature])])\n",
    "            data_vectors=np.concatenate((data_vectors, normValues.T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "abDF=pd.DataFrame()\n",
    "\n",
    "for vectorizer in [ablation1(), ablation2(), ablation3(), ablation4(), ablation5()]:\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 5\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print(\"\\t****************************************\\n\")\n",
    "    \n",
    "    entry={\"F1 Macro\":macro, \"F1 Micro\":micro}\n",
    "    abDF=abDF.append(entry, ignore_index=True)\n",
    "    \n",
    "abDF['Features Removed']=['Fall Duration', 'Metamap Location Tags', 'Metamap Description Tags', \n",
    "                          'TweetNLP Description Tags', 'N-Grams']\n",
    "abDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training size vs performance (F1 macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for frac in np.arange(.4, 1.01, .02):\n",
    "    \n",
    "    partial_training_set_size=int(frac*training_set_size)\n",
    "    partial_training_rows = training_rows.sample(n=partial_training_set_size)\n",
    "    partial_training_classes=partial_training_rows['target'].tolist()\n",
    "    \n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 5\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, partial_training_rows,partial_training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    x.append(partial_training_set_size)\n",
    "    y.append(f1(predictions_test,test_classes, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kite.com/python/answers/how-to-plot-a-linear-regression-line-on-a-scatter-plot-in-python\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.plot(x, [m*xi + b for xi in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioNLPEnv",
   "language": "python",
   "name": "bionlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}