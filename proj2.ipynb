{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys\\n!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\\n#'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "st = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def loadDataAsDataFrame(f_path):\n",
    "    '''\n",
    "        Given a path, loads a data set and puts it into a dataframe\n",
    "        - simplified mechanism\n",
    "    '''\n",
    "    df = pd.read_csv(f_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "\n",
    "    # Replace/remove username\n",
    "    raw_text = re.sub('(@[A-Za-z0-9\\_]+)', '_atUsername_', raw_text)\n",
    "    raw_text = re.sub('(http[A-Za-z0-9\\_]+)', '_website_', raw_text)\n",
    "    #stemming and lowercasing\n",
    "    words=[]\n",
    "    for w in raw_text.lower().split():\n",
    "        if not w in st and not w in ['.',',', '[', ']', '(', ')']:\n",
    "            words.append(w)\n",
    "            \n",
    "    return (\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1143</td>\n",
       "      <td>835999046204551168</td>\n",
       "      <td>2017-02-26 23:44:39</td>\n",
       "      <td>my mom could have worked while dying from stag...</td>\n",
       "      <td>0</td>\n",
       "      <td>mom could worked dying stage 4 breast cancer &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1981</td>\n",
       "      <td>839261636275933184</td>\n",
       "      <td>2017-03-07 23:49:01</td>\n",
       "      <td>new drug 4 breast cancer is $10k per month. Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>new drug 4 breast cancer $10k per month. affor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1153</td>\n",
       "      <td>853032936324464640</td>\n",
       "      <td>2017-04-14 23:51:15</td>\n",
       "      <td>When people who don't know me try to educate m...</td>\n",
       "      <td>1</td>\n",
       "      <td>people know try educate breast cancer patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1154</td>\n",
       "      <td>859919598904254464</td>\n",
       "      <td>2017-05-03 23:56:23</td>\n",
       "      <td>This 11 year breast cancer survivor needs heal...</td>\n",
       "      <td>0</td>\n",
       "      <td>11 year breast cancer survivor needs healthcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1159</td>\n",
       "      <td>861368530817732608</td>\n",
       "      <td>2017-05-07 23:53:55</td>\n",
       "      <td>@KellyMazeski Fellow breast cancer survivor he...</td>\n",
       "      <td>0</td>\n",
       "      <td>_atusername_ fellow breast cancer survivor don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>5991</td>\n",
       "      <td>5991</td>\n",
       "      <td>1235419985733918976</td>\n",
       "      <td>2020-03-05 04:20:52</td>\n",
       "      <td>b'just got back to bk and found out my aunt ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'just got back bk found aunt breast cancer da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>5992</td>\n",
       "      <td>5992</td>\n",
       "      <td>1224811121976271104</td>\n",
       "      <td>2020-02-04 21:45:02</td>\n",
       "      <td>b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>5993</td>\n",
       "      <td>5993</td>\n",
       "      <td>1234769787672302080</td>\n",
       "      <td>2020-03-03 09:17:12</td>\n",
       "      <td>b'@dailymailceleb @dailymailuk lovely see @kyl...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'_atusername_ _atusername_ lovely see _atuser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>5994</td>\n",
       "      <td>5994</td>\n",
       "      <td>1235541633812185088</td>\n",
       "      <td>2020-03-05 12:24:15</td>\n",
       "      <td>b'@drkcain as a breast cancer patient myself f...</td>\n",
       "      <td>1</td>\n",
       "      <td>b'_atusername_ breast cancer patient 27 yrs, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5995</td>\n",
       "      <td>5995</td>\n",
       "      <td>1225497981379579904</td>\n",
       "      <td>2020-02-06 19:14:22</td>\n",
       "      <td>b'@aasfoundation1 @claranlee @univsurg @gdkenn...</td>\n",
       "      <td>0</td>\n",
       "      <td>b'_atusername_ _atusername_ _atusername_ _atus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5019 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1                   ID            Timestamp  \\\n",
       "0              0          1143   835999046204551168  2017-02-26 23:44:39   \n",
       "1              2          1981   839261636275933184  2017-03-07 23:49:01   \n",
       "2              5          1153   853032936324464640  2017-04-14 23:51:15   \n",
       "3              7          1154   859919598904254464  2017-05-03 23:56:23   \n",
       "4              9          1159   861368530817732608  2017-05-07 23:53:55   \n",
       "...          ...           ...                  ...                  ...   \n",
       "5014        5991          5991  1235419985733918976  2020-03-05 04:20:52   \n",
       "5015        5992          5992  1224811121976271104  2020-02-04 21:45:02   \n",
       "5016        5993          5993  1234769787672302080  2020-03-03 09:17:12   \n",
       "5017        5994          5994  1235541633812185088  2020-03-05 12:24:15   \n",
       "5018        5995          5995  1225497981379579904  2020-02-06 19:14:22   \n",
       "\n",
       "                                                   Text  Class  \\\n",
       "0     my mom could have worked while dying from stag...      0   \n",
       "1     new drug 4 breast cancer is $10k per month. Ca...      0   \n",
       "2     When people who don't know me try to educate m...      1   \n",
       "3     This 11 year breast cancer survivor needs heal...      0   \n",
       "4     @KellyMazeski Fellow breast cancer survivor he...      0   \n",
       "...                                                 ...    ...   \n",
       "5014  b'just got back to bk and found out my aunt ha...      0   \n",
       "5015  b'#iamandiwill \\n\\ni am strong \\ni am loved\\ni...      1   \n",
       "5016  b'@dailymailceleb @dailymailuk lovely see @kyl...      0   \n",
       "5017  b'@drkcain as a breast cancer patient myself f...      1   \n",
       "5018  b'@aasfoundation1 @claranlee @univsurg @gdkenn...      0   \n",
       "\n",
       "                                     preprocessed_texts  \n",
       "0     mom could worked dying stage 4 breast cancer &...  \n",
       "1     new drug 4 breast cancer $10k per month. affor...  \n",
       "2     people know try educate breast cancer patient ...  \n",
       "3     11 year breast cancer survivor needs healthcar...  \n",
       "4     _atusername_ fellow breast cancer survivor don...  \n",
       "...                                                 ...  \n",
       "5014  b'just got back bk found aunt breast cancer da...  \n",
       "5015  b'#iamandiwill \\n\\ni strong \\ni loved\\ni alive...  \n",
       "5016  b'_atusername_ _atusername_ lovely see _atuser...  \n",
       "5017  b'_atusername_ breast cancer patient 27 yrs, t...  \n",
       "5018  b'_atusername_ _atusername_ _atusername_ _atus...  \n",
       "\n",
       "[5019 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "f_path = './Breast Cancer(Raw_data_2_Classes).csv'\n",
    "data = loadDataAsDataFrame(f_path)\n",
    "\n",
    "texts = data['Text']\n",
    "classes = data['Class']\n",
    "ids = data['ID']\n",
    "\n",
    "#PREPROCESS THE DATA\n",
    "texts_preprocessed=[preprocess_text(txt) for txt in texts]\n",
    "data['preprocessed_texts']=texts_preprocessed\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This 11 year breast cancer survivor needs healthcare!  Medicare for all!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3736, 1: 1283})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2017-02-26 23:44:39\n",
       "1       2017-03-07 23:49:01\n",
       "2       2017-04-14 23:51:15\n",
       "3       2017-05-03 23:56:23\n",
       "4       2017-05-07 23:53:55\n",
       "               ...         \n",
       "5014    2020-03-05 04:20:52\n",
       "5015    2020-02-04 21:45:02\n",
       "5016    2020-03-03 09:17:12\n",
       "5017    2020-03-05 12:24:15\n",
       "5018    2020-02-06 19:14:22\n",
       "Name: Timestamp, Length: 5019, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import ciso8601\n",
    "\n",
    "def dateToBetterDatetime(string):\n",
    "    return ciso8601.parse_datetime(string)#.date()\n",
    "\n",
    "#returns how many minutes have passed since midnight\n",
    "def timeOfDay(dtObject):\n",
    "    tim=dtObject - datetime.datetime(dtObject.year, dtObject.month, dtObject.day)\n",
    "    return tim.seconds // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-26 23:44:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1424"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['Timestamp'][0])\n",
    "date=dateToBetterDatetime(data['Timestamp'][0])\n",
    "timeOfDay(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what about tfidf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "tfidfvectorizer.fit(texts_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x14147 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvectorizer.transform(texts_preprocessed[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_clusters = {}\n",
    "\n",
    "def loadwordclusters():\n",
    "    infile = open('./50mpaths2',  \"r\", encoding=\"utf-8\")\n",
    "    for line in infile:\n",
    "        items = str.strip(line).split()\n",
    "        class_ = items[0]\n",
    "        term = items[1]\n",
    "        word_clusters[term] = class_\n",
    "    return word_clusters\n",
    "\n",
    "def getclusterfeatures(sent):\n",
    "    sent = sent.lower()\n",
    "    terms = nltk.word_tokenize(sent)\n",
    "    cluster_string = ''\n",
    "    for t in terms:\n",
    "        if t in word_clusters.keys():\n",
    "                cluster_string += 'clust_' + word_clusters[t] + '_clust '\n",
    "    return str.strip(cluster_string)\n",
    "\n",
    "loadwordclusters()\n",
    "\n",
    "class myVectorizer():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "        self.tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.tfidfvectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        tfidf_vectors=self.tfidfvectorizer.transform(texts_preprocessed).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, tfidf_vectors), axis=1)\n",
    "        \n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "#favor precision 2x more than recall\n",
    "fhalf_scorer = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hyperparam_space(params, pipeline, folds, training_texts, training_classes):#folds, x_train, y_train, x_validation, y_validation):\n",
    "        grid_search = GridSearchCV(estimator=pipeline, param_grid=params, refit=True, cv=folds, return_train_score=False, scoring='f1_macro',n_jobs=-1)\n",
    "        grid_search.fit(training_texts, training_classes)\n",
    "        return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set_size = int(0.8*len(data))\n",
    "\n",
    "X=data\n",
    "y=data['Class'].tolist()\n",
    "\n",
    "training_rows, test_rows, training_classes, test_classes = train_test_split(\n",
    "    X, y, train_size=training_set_size, random_state=42069)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(value, maxOfList, minOfList):\n",
    "    return (value - minOfList) / (maxOfList - minOfList)\n",
    "    \n",
    "def getNormalizedList(values, maxOfList, minOfList):\n",
    "    ret = []\n",
    "    for value in values:\n",
    "        ret.append(normalize(value, maxOfList, minOfList))\n",
    "        \n",
    "    return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf1\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def bulkEval(predictions_test, test_classes, bs=False):\n",
    "    print (\"Accuracy\\t\", acc(predictions_test,test_classes))\n",
    "    macro=f1(predictions_test,test_classes, average='macro')\n",
    "    micro=f1(predictions_test,test_classes, average='micro')\n",
    "    FHalf=fbeta_score(predictions_test, test_classes, beta=0.5)\n",
    "    print (\"F1 Macro\\t\", macro)\n",
    "    print (\"F1 Micro\\t\", micro)\n",
    "    print (\"FHalf\\t\", FHalf)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(test_classes, predictions_test, labels=[1,0], normalize='true'))\n",
    "\n",
    "    #bootstrap it\n",
    "    interval=None\n",
    "    if bs:\n",
    "        f1s=[]\n",
    "        fHalves=[]\n",
    "        for iteration in range(1000):\n",
    "            resampleIndexes=random.choices(range(len(predictions_test)), k=1000)\n",
    "            resamplePreds=[predictions_test[i] for i in resampleIndexes]\n",
    "            resampleTrueClasses=[test_classes[i] for i in resampleIndexes]\n",
    "            thisF1=f1(resamplePreds,resampleTrueClasses, average='macro')\n",
    "            thisFHalf=fbeta_score(resamplePreds,resampleTrueClasses, beta=0.5)\n",
    "            fHalves.append(thisFHalf)\n",
    "            f1s.append(thisF1)\n",
    "\n",
    "        print(\"Bootstrapping 95% confidence interval for F1:\")\n",
    "        interval=np.percentile(f1s, [2.5, 97.5])\n",
    "        print(interval)\n",
    "        print(\"Bootstrapping 95% confidence interval for FHalf:\")\n",
    "        halfinterval=np.percentile(fHalves, [2.5, 97.5])\n",
    "        print(halfinterval)\n",
    "\n",
    "    print(\"\\t****************************************\\n\")\n",
    "    \n",
    "    return macro, micro, interval, FHalf, halfinterval\n",
    "\n",
    "    #entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "    #          \"Confidence Interval\":interval}\n",
    "    #f1df=f1df.append(entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess 0 Classifier\n",
    "The Stupidest classifier possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stupidClassifier():\n",
    "    def fit(self, X,y):\n",
    "        doNothing=True\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ret = []\n",
    "        for xi in X.iterrows():\n",
    "            ret.append(0)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.7609561752988048\n",
      "F1 Macro\t 0.4321266968325792\n",
      "F1 Micro\t 0.7609561752988048\n",
      "FHalf\t 0.0\n",
      "Confusion Matrix\n",
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.42329873 0.44009742]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0. 0.]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4321266968325792,\n",
       " 0.7609561752988048,\n",
       " array([0.42329873, 0.44009742]),\n",
       " 0.0,\n",
       " array([0., 0.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLASSIFIER\n",
    "clf=stupidClassifier()\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = clf.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = myVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNB baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.6444223107569721\n",
      "F1 Macro\t 0.586774591850734\n",
      "F1 Micro\t 0.6444223107569721\n",
      "FHalf\t 0.5040770941438102\n",
      "Confusion Matrix\n",
      "[[0.56666667 0.43333333]\n",
      " [0.33115183 0.66884817]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.55275705 0.61877457]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.44270445 0.56138875]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.586774591850734,\n",
       " 0.6444223107569721,\n",
       " array([0.55275705, 0.61877457]),\n",
       " 0.5040770941438102,\n",
       " array([0.44270445, 0.56138875]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#CLASSIFIER\n",
    "gnb_classifier = GaussianNB()\n",
    "grid_params = {}\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',gnb_classifier)])\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 5\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes, bs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple transformer RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "model_args={'overwrite_output_dir':True}\n",
    "\n",
    "# Create a TransformerModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', use_cuda=True, args=model_args)\n",
    "#model = ClassificationModel('roberta', 'roberta-base', use_cuda=True, args=model_args)\n",
    "\n",
    "#change our data into a format that simpletransformers can process\n",
    "training_rows['text']=training_rows['Text']\n",
    "training_rows['labels']=training_rows['Class']\n",
    "test_rows['text']=test_rows['Text']\n",
    "test_rows['labels']=test_rows['Class']\n",
    "\n",
    "# Train the model\n",
    "model.train_model(training_rows)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_rows)\n",
    "\n",
    "print(\"f1 score\")\n",
    "precision=result['tp'] / (result['tp'] + result['fp'])\n",
    "recall=result['tp'] / (result['tp'] + result['fn'])\n",
    "f1score= 2 * precision * recall / (precision + recall)\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "\n",
    "Best hyperparameters:\n",
    "{'svm_classifier__C': 4, 'svm_classifier__kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "#CLASSIFIER\n",
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [0.25,1,4,16,64],\n",
    "     'svm_classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('svm_classifier',svm_classifier)])\n",
    "\n",
    "grid_params = {\n",
    "     'svm_classifier__C': [4],\n",
    "     'svm_classifier__kernel': ['rbf'],\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(bulkEval(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_estimators': 5}\n",
    "\n",
    "35 for f-half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__n_estimators': 35}\n",
      "Optimal n found: 35\n",
      "Performance on held-out test set ... :\n",
      "0.8137450199203188\n"
     ]
    }
   ],
   "source": [
    "vectorizer = myVectorizer()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',rf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_estimators': np.arange(5,60,5)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "print('Optimal n found:', grid.best_params_['classifier__n_estimators'])\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__n_neighbors': 3}\n",
    "9 for f-half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__n_neighbors': 9}\n",
      "Performance on held-out test set ... :\n",
      "0.7798804780876494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= KNeighborsClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "grid_params = {\n",
    "     'classifier__n_neighbors': np.arange(1,20,1),\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Is it really data science if there isn't a neural network somewhere?\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__hidden_layer_sizes': (11,)}\n",
    "\n",
    "71 for fhalf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__hidden_layer_sizes': (71,)}\n",
      "Performance on held-out test set ... :\n",
      "0.8227091633466136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "vectorizer = myVectorizer()\n",
    "\n",
    "clf= MLPClassifier()\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     'classifier__hidden_layer_sizes': layerParams,\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "print(accuracy_score(predictions_test,test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaboost\n",
    "\n",
    "Best hyperparameters:\n",
    "{'classifier__base_estimator__max_depth': 3, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}\n",
    "\n",
    "for fhalf\n",
    "Best hyperparameters:\n",
    "{'classifier__base_estimator__max_depth': 2, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'classifier__base_estimator__max_depth': 2, 'classifier__base_estimator__n_estimators': 30, 'classifier__n_estimators': 50}\n",
      "Performance on held-out test set ... :\n",
      "Accuracy\t 0.8456175298804781\n",
      "F1 Macro\t 0.754811366089224\n",
      "F1 Micro\t 0.8456175298804781\n",
      "Confusion Matrix\n",
      "[[0.49583333 0.50416667]\n",
      " [0.04450262 0.95549738]]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.754811366089224, 0.8456175298804781, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(), random_state=420)\n",
    "\n",
    "#SIMPLE PIPELINE\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "#pipeline ensures vectorization happens in each fold of grid search \n",
    "#(you could code the entire process manually for more flexibility)\n",
    "\n",
    "#we'll just use one hidden layer\n",
    "layerParams=[]\n",
    "for n in range(1,101, 10):\n",
    "    layerParams.append(tuple([n]))\n",
    "    \n",
    "grid_params = {\n",
    "     \"classifier__base_estimator__n_estimators\":range(10, 51, 20),\n",
    "     \"classifier__base_estimator__max_depth\":range(3),\n",
    "    \"classifier__n_estimators\":range(10, 51, 20)\n",
    "}\n",
    "\n",
    "#SEARCH HYPERPARAMETERS\n",
    "folds = 2\n",
    "grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "#CLASSIFY AND EVALUATE \n",
    "predictions_test = grid.predict(test_rows)\n",
    "print('Performance on held-out test set ... :')\n",
    "\n",
    "bulkEval(predictions_test,test_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5345911949685536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now evaluate them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t GaussianNB()\n",
      "Accuracy\t 0.6444223107569721\n",
      "F1 Macro\t 0.586774591850734\n",
      "F1 Micro\t 0.6444223107569721\n",
      "FHalf\t 0.5040770941438102\n",
      "Confusion Matrix\n",
      "[[0.56666667 0.43333333]\n",
      " [0.33115183 0.66884817]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.55743126 0.61793022]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.45418486 0.55259402]\n",
      "\t****************************************\n",
      "\n",
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t RandomForestClassifier(n_estimators=5)\n",
      "Accuracy\t 0.7908366533864541\n",
      "F1 Macro\t 0.6805047915189205\n",
      "F1 Micro\t 0.7908366533864541\n",
      "FHalf\t 0.44973544973544977\n",
      "Confusion Matrix\n",
      "[[0.425      0.575     ]\n",
      " [0.09424084 0.90575916]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.64587394 0.71672396]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.38851808 0.51212186]\n",
      "\t****************************************\n",
      "\n",
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t KNeighborsClassifier(n_neighbors=3)\n",
      "Accuracy\t 0.7529880478087649\n",
      "F1 Macro\t 0.6292643415284924\n",
      "F1 Micro\t 0.7529880478087649\n",
      "FHalf\t 0.3846153846153846\n",
      "Confusion Matrix\n",
      "[[0.36666667 0.63333333]\n",
      " [0.12565445 0.87434555]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.59286082 0.66661621]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.32172318 0.44427409]\n",
      "\t****************************************\n",
      "\n",
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t MLPClassifier(hidden_layer_sizes=(10,))\n",
      "Accuracy\t 0.8256972111553785\n",
      "F1 Macro\t 0.7607854189386606\n",
      "F1 Micro\t 0.8256972111553785\n",
      "FHalf\t 0.6369691923397168\n",
      "Confusion Matrix\n",
      "[[0.6375     0.3625    ]\n",
      " [0.11518325 0.88481675]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.72912424 0.79079338]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.58181583 0.68848108]\n",
      "\t****************************************\n",
      "\n",
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=3,\n",
      "                                                         n_estimators=30))\n",
      "Accuracy\t 0.8555776892430279\n",
      "F1 Macro\t 0.7749279160192636\n",
      "F1 Micro\t 0.8555776892430279\n",
      "FHalf\t 0.5743544078361531\n",
      "Confusion Matrix\n",
      "[[0.5375     0.4625    ]\n",
      " [0.04450262 0.95549738]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.74112858 0.80861521]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.51291845 0.63508101]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>F-1/2</th>\n",
       "      <th>F-1/2 Confidence Interval</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.504077</td>\n",
       "      <td>[0.45418486436442534, 0.5525940203217854]</td>\n",
       "      <td>[0.5574312640361156, 0.6179302245897679]</td>\n",
       "      <td>0.586775</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(n_estimators=5)</td>\n",
       "      <td>0.449735</td>\n",
       "      <td>[0.38851807725832144, 0.5121218554556943]</td>\n",
       "      <td>[0.6458739441195582, 0.7167239556040689]</td>\n",
       "      <td>0.680505</td>\n",
       "      <td>0.790837</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[0.3217231751096929, 0.4442740916828397]</td>\n",
       "      <td>[0.5928608237574813, 0.6666162147885939]</td>\n",
       "      <td>0.629264</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(10,))</td>\n",
       "      <td>0.636969</td>\n",
       "      <td>[0.5818158326798166, 0.6884810756972113]</td>\n",
       "      <td>[0.7291242424187502, 0.7907933766726869]</td>\n",
       "      <td>0.760785</td>\n",
       "      <td>0.825697</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier(base_estimator=RandomForest...</td>\n",
       "      <td>0.574354</td>\n",
       "      <td>[0.5129184508509322, 0.6350810149432387]</td>\n",
       "      <td>[0.7411285789032661, 0.8086152139486253]</td>\n",
       "      <td>0.774928</td>\n",
       "      <td>0.855578</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Classifier     F-1/2  \\\n",
       "0                                       GaussianNB()  0.504077   \n",
       "1             RandomForestClassifier(n_estimators=5)  0.449735   \n",
       "2                KNeighborsClassifier(n_neighbors=3)  0.384615   \n",
       "3            MLPClassifier(hidden_layer_sizes=(10,))  0.636969   \n",
       "4  AdaBoostClassifier(base_estimator=RandomForest...  0.574354   \n",
       "\n",
       "                   F-1/2 Confidence Interval  \\\n",
       "0  [0.45418486436442534, 0.5525940203217854]   \n",
       "1  [0.38851807725832144, 0.5121218554556943]   \n",
       "2   [0.3217231751096929, 0.4442740916828397]   \n",
       "3   [0.5818158326798166, 0.6884810756972113]   \n",
       "4   [0.5129184508509322, 0.6350810149432387]   \n",
       "\n",
       "                     F1 Confidence Interval  F1 Macro  F1 Micro  Precision  \n",
       "0  [0.5574312640361156, 0.6179302245897679]  0.586775  0.644422   0.566667  \n",
       "1  [0.6458739441195582, 0.7167239556040689]  0.680505  0.790837   0.425000  \n",
       "2  [0.5928608237574813, 0.6666162147885939]  0.629264  0.752988   0.366667  \n",
       "3  [0.7291242424187502, 0.7907933766726869]  0.760785  0.825697   0.637500  \n",
       "4  [0.7411285789032661, 0.8086152139486253]  0.774928  0.855578   0.537500  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Confusion matrix whose i-th row and j-th column entry indicates the number of samples \n",
    "with true label being i-th class and prediced label being j-th class.\n",
    "\"\"\"\n",
    "\n",
    "gnb = GaussianNB()\n",
    "#svmc = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "nn=MLPClassifier(hidden_layer_sizes=(10,))\n",
    "ada=AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "en=VotingClassifier(estimators=[ ('RF', rf), #('SVM', svmc),\n",
    "                                (\"KNN\", knn), (\"NN\", nn)], \n",
    "                                      voting='soft')\n",
    "\n",
    "f1df=pd.DataFrame()\n",
    "\n",
    "for clf in [gnb, rf, knn, nn, ada]:\n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 10\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    print('Best hyperparameters:')\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    print(\"Classifier\\t\", clf)\n",
    "\n",
    "    macro, micro, interval, fhalf, halfinterval=bulkEval(predictions_test, test_classes, bs=True)\n",
    "    prec=precision_score(predictions_test, test_classes)\n",
    "    \n",
    "    entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"F1 Confidence Interval\":interval, \"Precision\":prec,\n",
    "          \"F-1/2\": fhalf, \"F-1/2 Confidence Interval\": halfinterval}\n",
    "    f1df=f1df.append(entry, ignore_index=True)\n",
    "    \n",
    "f1df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t SVC(C=4, probability=True)\n",
      "Accuracy\t 0.8545816733067729\n",
      "F1 Macro\t 0.7915804081261233\n",
      "F1 Micro\t 0.854581673306773\n",
      "FHalf\t 0.6527303754266212\n",
      "Confusion Matrix\n",
      "[[0.6375     0.3625    ]\n",
      " [0.07722513 0.92277487]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.76064019 0.81964789]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.59671554 0.7038873 ]\n",
      "\t****************************************\n",
      "\n",
      "Best hyperparameters:\n",
      "{}\n",
      "Classifier\t VotingClassifier(estimators=[('RF', RandomForestClassifier(n_estimators=5)),\n",
      "                             ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
      "                             ('NN', MLPClassifier(hidden_layer_sizes=(11,)))],\n",
      "                 voting='soft')\n",
      "Accuracy\t 0.8396414342629482\n",
      "F1 Macro\t 0.757215102036525\n",
      "F1 Micro\t 0.8396414342629482\n",
      "FHalf\t 0.5662862159789289\n",
      "Confusion Matrix\n",
      "[[0.5375     0.4625    ]\n",
      " [0.06544503 0.93455497]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.72396188 0.78925013]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.50734319 0.62441853]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>F-1/2</th>\n",
       "      <th>F-1/2 Confidence Interval</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.504077</td>\n",
       "      <td>[0.45418486436442534, 0.5525940203217854]</td>\n",
       "      <td>[0.5574312640361156, 0.6179302245897679]</td>\n",
       "      <td>0.586775</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(n_estimators=5)</td>\n",
       "      <td>0.449735</td>\n",
       "      <td>[0.38851807725832144, 0.5121218554556943]</td>\n",
       "      <td>[0.6458739441195582, 0.7167239556040689]</td>\n",
       "      <td>0.680505</td>\n",
       "      <td>0.790837</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[0.3217231751096929, 0.4442740916828397]</td>\n",
       "      <td>[0.5928608237574813, 0.6666162147885939]</td>\n",
       "      <td>0.629264</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(10,))</td>\n",
       "      <td>0.636969</td>\n",
       "      <td>[0.5818158326798166, 0.6884810756972113]</td>\n",
       "      <td>[0.7291242424187502, 0.7907933766726869]</td>\n",
       "      <td>0.760785</td>\n",
       "      <td>0.825697</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier(base_estimator=RandomForest...</td>\n",
       "      <td>0.574354</td>\n",
       "      <td>[0.5129184508509322, 0.6350810149432387]</td>\n",
       "      <td>[0.7411285789032661, 0.8086152139486253]</td>\n",
       "      <td>0.774928</td>\n",
       "      <td>0.855578</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(C=4, probability=True)</td>\n",
       "      <td>0.652730</td>\n",
       "      <td>[0.5967155354326794, 0.7038873046279768]</td>\n",
       "      <td>[0.7606401945755146, 0.819647886952278]</td>\n",
       "      <td>0.791580</td>\n",
       "      <td>0.854582</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VotingClassifier(estimators=[('RF', RandomFore...</td>\n",
       "      <td>0.566286</td>\n",
       "      <td>[0.5073431884388397, 0.624418531340504]</td>\n",
       "      <td>[0.723961882516624, 0.7892501336599992]</td>\n",
       "      <td>0.757215</td>\n",
       "      <td>0.839641</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Classifier     F-1/2  \\\n",
       "0                                       GaussianNB()  0.504077   \n",
       "1             RandomForestClassifier(n_estimators=5)  0.449735   \n",
       "2                KNeighborsClassifier(n_neighbors=3)  0.384615   \n",
       "3            MLPClassifier(hidden_layer_sizes=(10,))  0.636969   \n",
       "4  AdaBoostClassifier(base_estimator=RandomForest...  0.574354   \n",
       "5                         SVC(C=4, probability=True)  0.652730   \n",
       "6  VotingClassifier(estimators=[('RF', RandomFore...  0.566286   \n",
       "\n",
       "                   F-1/2 Confidence Interval  \\\n",
       "0  [0.45418486436442534, 0.5525940203217854]   \n",
       "1  [0.38851807725832144, 0.5121218554556943]   \n",
       "2   [0.3217231751096929, 0.4442740916828397]   \n",
       "3   [0.5818158326798166, 0.6884810756972113]   \n",
       "4   [0.5129184508509322, 0.6350810149432387]   \n",
       "5   [0.5967155354326794, 0.7038873046279768]   \n",
       "6    [0.5073431884388397, 0.624418531340504]   \n",
       "\n",
       "                     F1 Confidence Interval  F1 Macro  F1 Micro  Precision  \n",
       "0  [0.5574312640361156, 0.6179302245897679]  0.586775  0.644422   0.566667  \n",
       "1  [0.6458739441195582, 0.7167239556040689]  0.680505  0.790837   0.425000  \n",
       "2  [0.5928608237574813, 0.6666162147885939]  0.629264  0.752988   0.366667  \n",
       "3  [0.7291242424187502, 0.7907933766726869]  0.760785  0.825697   0.637500  \n",
       "4  [0.7411285789032661, 0.8086152139486253]  0.774928  0.855578   0.537500  \n",
       "5   [0.7606401945755146, 0.819647886952278]  0.791580  0.854582   0.637500  \n",
       "6   [0.723961882516624, 0.7892501336599992]  0.757215  0.839641   0.537500  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "svmc = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "nn=MLPClassifier(hidden_layer_sizes=(11,))\n",
    "en=VotingClassifier(estimators=[ ('RF', rf), #('SVM', svmc),\n",
    "                                (\"KNN\", knn), (\"NN\", nn)], \n",
    "                                      voting='soft')\n",
    "\n",
    "for clf in [svmc, en]:\n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 10\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    print('Best hyperparameters:')\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    print(\"Classifier\\t\", clf)\n",
    "\n",
    "    macro, micro, interval, fhalf, halfinterval=bulkEval(predictions_test, test_classes, bs=True)\n",
    "    prec=precision_score(predictions_test, test_classes)\n",
    "    \n",
    "    entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"F1 Confidence Interval\":interval, \"Precision\":prec,\n",
    "          \"F-1/2\": fhalf, \"F-1/2 Confidence Interval\": halfinterval}\n",
    "    f1df=f1df.append(entry, ignore_index=True)\n",
    "    \n",
    "f1df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1df.to_pickle(\"clfScores.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f1df=pd.read_pickle(\"clfScores.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the precision function isn't working right\n",
    "\n",
    "i don't know why\n",
    "\n",
    " I'll just make the precisions manually from the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs=[.56/.9, .425/.52, .366/.49, .64/.75, .54/.58, .64/.71, .54/.6]\n",
    "f1df['Precision']=precs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundToHundredths(num):\n",
    "    return round(num * 100) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1ci = f1df['F1 Confidence Interval']\n",
    "f1ci=[[roundToHundredths(a), roundToHundredths(b)] for [a,b] in f1ci]\n",
    "f1df['F1 Confidence Interval']=f1ci\n",
    "\n",
    "f1df['F1 Macro'] = [roundToHundredths(x) for x in f1df['F1 Macro']]\n",
    "f1df['F1 Micro'] = [roundToHundredths(x) for x in f1df['F1 Micro']]\n",
    "f1df['Precision'] = [roundToHundredths(x) for x in f1df['Precision']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[0.56, 0.62]</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(n_estimators=5)</td>\n",
       "      <td>[0.65, 0.72]</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>[0.59, 0.67]</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(10,))</td>\n",
       "      <td>[0.73, 0.79]</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier(base_estimator=RandomForest...</td>\n",
       "      <td>[0.74, 0.81]</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(C=4, probability=True)</td>\n",
       "      <td>[0.76, 0.82]</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VotingClassifier(estimators=[('RF', RandomFore...</td>\n",
       "      <td>[0.72, 0.79]</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Classifier F1 Confidence Interval  \\\n",
       "0                                       GaussianNB()           [0.56, 0.62]   \n",
       "1             RandomForestClassifier(n_estimators=5)           [0.65, 0.72]   \n",
       "2                KNeighborsClassifier(n_neighbors=3)           [0.59, 0.67]   \n",
       "3            MLPClassifier(hidden_layer_sizes=(10,))           [0.73, 0.79]   \n",
       "4  AdaBoostClassifier(base_estimator=RandomForest...           [0.74, 0.81]   \n",
       "5                         SVC(C=4, probability=True)           [0.76, 0.82]   \n",
       "6  VotingClassifier(estimators=[('RF', RandomFore...           [0.72, 0.79]   \n",
       "\n",
       "   F1 Macro  F1 Micro  Precision  \n",
       "0      0.59      0.64       0.62  \n",
       "1      0.68      0.79       0.82  \n",
       "2      0.63      0.75       0.75  \n",
       "3      0.76      0.83       0.85  \n",
       "4      0.77      0.86       0.93  \n",
       "5      0.79      0.85       0.90  \n",
       "6      0.76      0.84       0.90  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1df[['Classifier', 'F1 Confidence Interval', 'F1 Macro', 'F1 Micro', 'Precision']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myVectorizer001():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.tfidfvectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        tfidf_vectors=self.tfidfvectorizer.transform(texts_preprocessed).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, tfidf_vectors), axis=1)\n",
    "        \n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "class myVectorizer002():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        #self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        #clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            #clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.tfidfvectorizer.fit(texts_preprocessed)\n",
    "        #self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        #clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            #clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        tfidf_vectors=self.tfidfvectorizer.transform(texts_preprocessed).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, tfidf_vectors), axis=1)\n",
    "        \n",
    "        #cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        #data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "class myVectorizer003():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        #self.tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        #self.tfidfvectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        #tfidf_vectors=self.tfidfvectorizer.transform(texts_preprocessed).toarray()\n",
    "        #data_vectors = np.concatenate((data_vectors, tfidf_vectors), axis=1)\n",
    "        \n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "class myVectorizer004():\n",
    "    def __init__(self):\n",
    "        #self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "        \n",
    "        #for normaluzation\n",
    "        self.maxs={}\n",
    "        self.mins={}\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        textLens=[]\n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "            \n",
    "        \n",
    "        #self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.tfidfvectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "        \n",
    "        self.maxs['len']=max(textLens)\n",
    "        self.mins['len']=min(textLens)\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        textLens=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            textLens.append(len(word_tokenize(tr)))\n",
    "        \n",
    "        #data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        tfidf_vectors=self.tfidfvectorizer.transform(texts_preprocessed).toarray()\n",
    "        data_vectors = tfidf_vectors#np.concatenate((data_vectors, tfidf_vectors), axis=1)\n",
    "        \n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        textLensNorm=getNormalizedList(textLens, self.maxs['len'], self.mins['len'])\n",
    "        data_vectors = np.concatenate((data_vectors, np.array([textLensNorm]).T), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\t 0.8585657370517928\n",
      "F1 Macro\t 0.7775086613190174\n",
      "F1 Micro\t 0.8585657370517928\n",
      "FHalf\t 0.5724508050089445\n",
      "Confusion Matrix\n",
      "[[0.53333333 0.46666667]\n",
      " [0.03926702 0.96073298]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.74470393 0.80694154]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.51357927 0.62665771]\n",
      "\t****************************************\n",
      "\n",
      "Accuracy\t 0.8147410358565738\n",
      "F1 Macro\t 0.6865933151185194\n",
      "F1 Micro\t 0.8147410358565738\n",
      "FHalf\t 0.4066543438077634\n",
      "Confusion Matrix\n",
      "[[0.36666667 0.63333333]\n",
      " [0.04450262 0.95549738]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.6495853  0.72059004]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.34482328 0.46763053]\n",
      "\t****************************************\n",
      "\n",
      "Accuracy\t 0.8565737051792829\n",
      "F1 Macro\t 0.7717720751667003\n",
      "F1 Micro\t 0.8565737051792828\n",
      "FHalf\t 0.5575539568345323\n",
      "Confusion Matrix\n",
      "[[0.51666667 0.48333333]\n",
      " [0.03664921 0.96335079]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.7361446  0.80267375]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.49364641 0.61700231]\n",
      "\t****************************************\n",
      "\n",
      "Accuracy\t 0.8635458167330677\n",
      "F1 Macro\t 0.7849357455903759\n",
      "F1 Micro\t 0.8635458167330677\n",
      "FHalf\t 0.5819158460161146\n",
      "Confusion Matrix\n",
      "[[0.54166667 0.45833333]\n",
      " [0.03534031 0.96465969]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.75326777 0.815616  ]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.52338667 0.64092249]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-1/2</th>\n",
       "      <th>F-1/2 Confidence Interval</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572451</td>\n",
       "      <td>[0.5135792749087205, 0.6266577060559199]</td>\n",
       "      <td>[0.7447039328320719, 0.8069415422765013]</td>\n",
       "      <td>0.777509</td>\n",
       "      <td>0.858566</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>&lt;__main__.myVectorizer001 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.406654</td>\n",
       "      <td>[0.34482328016810776, 0.4676305307685182]</td>\n",
       "      <td>[0.6495853047534709, 0.7205900413345668]</td>\n",
       "      <td>0.686593</td>\n",
       "      <td>0.814741</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>&lt;__main__.myVectorizer002 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557554</td>\n",
       "      <td>[0.4936464081959879, 0.6170023143646193]</td>\n",
       "      <td>[0.7361446008915001, 0.8026737487854864]</td>\n",
       "      <td>0.771772</td>\n",
       "      <td>0.856574</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>&lt;__main__.myVectorizer003 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.581916</td>\n",
       "      <td>[0.5233866713557235, 0.6409224857316485]</td>\n",
       "      <td>[0.7532677657041373, 0.8156159978385782]</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.863546</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>&lt;__main__.myVectorizer004 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      F-1/2                  F-1/2 Confidence Interval  \\\n",
       "0  0.572451   [0.5135792749087205, 0.6266577060559199]   \n",
       "1  0.406654  [0.34482328016810776, 0.4676305307685182]   \n",
       "2  0.557554   [0.4936464081959879, 0.6170023143646193]   \n",
       "3  0.581916   [0.5233866713557235, 0.6409224857316485]   \n",
       "\n",
       "                     F1 Confidence Interval  F1 Macro  F1 Micro  Precision  \\\n",
       "0  [0.7447039328320719, 0.8069415422765013]  0.777509  0.858566   0.533333   \n",
       "1  [0.6495853047534709, 0.7205900413345668]  0.686593  0.814741   0.366667   \n",
       "2  [0.7361446008915001, 0.8026737487854864]  0.771772  0.856574   0.516667   \n",
       "3  [0.7532677657041373, 0.8156159978385782]  0.784936  0.863546   0.541667   \n",
       "\n",
       "                                          Vectorizer  \n",
       "0  <__main__.myVectorizer001 object at 0x000002A8...  \n",
       "1  <__main__.myVectorizer002 object at 0x000002A8...  \n",
       "2  <__main__.myVectorizer003 object at 0x000002A8...  \n",
       "3  <__main__.myVectorizer004 object at 0x000002A8...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#clf=svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "\n",
    "\n",
    "abDF=pd.DataFrame()\n",
    "\n",
    "for vectorizer in [myVectorizer001(),myVectorizer002(),myVectorizer003(),myVectorizer004()]:\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 10\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    macro, micro, interval, fhalf, halfinterval=bulkEval(predictions_test, test_classes, bs=True)\n",
    "    prec=precision_score(predictions_test, test_classes)\n",
    "    \n",
    "    entry={\"Vectorizer\": vectorizer, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"F1 Confidence Interval\":interval, \"Precision\":prec,\n",
    "          \"F-1/2\": fhalf, \"F-1/2 Confidence Interval\": halfinterval}\n",
    "    abDF=abDF.append(entry, ignore_index=True)\n",
    "    \n",
    "abDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myVectorizer005():\n",
    "    def __init__(self):\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "            \n",
    "        self.clustervectorizer.fit(clusters) \n",
    "\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.clustervectorizer.transform(clusters).toarray()        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)\n",
    "    \n",
    "class myVectorizer006():\n",
    "    def __init__(self):\n",
    "        self.textVectorizer=CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "        self.clustervectorizer = CountVectorizer(ngram_range=(1,1), max_features=1000)\n",
    "    \n",
    "    def fit(self, rows, y=None):\n",
    "        \n",
    "        #fall description\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))            \n",
    "        \n",
    "        self.textVectorizer.fit(texts_preprocessed)\n",
    "        self.clustervectorizer.fit(clusters) \n",
    "\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        unprocessedTexts=rows['Text']\n",
    "        \n",
    "        texts_preprocessed = []\n",
    "        clusters=[]\n",
    "        for tr in unprocessedTexts:\n",
    "            # you can do more with the training text here and generate more features...\n",
    "            texts_preprocessed.append(preprocess_text(tr))\n",
    "            clusters.append(getclusterfeatures(tr))\n",
    "        \n",
    "        data_vectors = self.textVectorizer.transform(texts_preprocessed).toarray()\n",
    "        \n",
    "        cluster_vectors = self.clustervectorizer.transform(clusters).toarray()\n",
    "        data_vectors = np.concatenate((data_vectors, cluster_vectors), axis=1)\n",
    "        \n",
    "        return data_vectors\n",
    "    \n",
    "    def fit_transform(self, rows, y=None):\n",
    "        self.fit(rows)\n",
    "        return self.transform(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\t 0.8356573705179283\n",
      "F1 Macro\t 0.7448302950486356\n",
      "F1 Micro\t 0.8356573705179283\n",
      "FHalf\t 0.5333333333333333\n",
      "Confusion Matrix\n",
      "[[0.5        0.5       ]\n",
      " [0.05890052 0.94109948]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.71188847 0.77746792]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.47523073 0.59475322]\n",
      "\t****************************************\n",
      "\n",
      "Accuracy\t 0.851593625498008\n",
      "F1 Macro\t 0.7652051762970171\n",
      "F1 Micro\t 0.851593625498008\n",
      "FHalf\t 0.5515695067264574\n",
      "Confusion Matrix\n",
      "[[0.5125     0.4875    ]\n",
      " [0.04188482 0.95811518]]\n",
      "Bootstrapping 95% confidence interval for F1:\n",
      "[0.73251177 0.79622433]\n",
      "Bootstrapping 95% confidence interval for FHalf:\n",
      "[0.49408278 0.60978738]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-1/2</th>\n",
       "      <th>F-1/2 Confidence Interval</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572451</td>\n",
       "      <td>[0.5135792749087205, 0.6266577060559199]</td>\n",
       "      <td>[0.7447039328320719, 0.8069415422765013]</td>\n",
       "      <td>0.777509</td>\n",
       "      <td>0.858566</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>&lt;__main__.myVectorizer001 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.406654</td>\n",
       "      <td>[0.34482328016810776, 0.4676305307685182]</td>\n",
       "      <td>[0.6495853047534709, 0.7205900413345668]</td>\n",
       "      <td>0.686593</td>\n",
       "      <td>0.814741</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>&lt;__main__.myVectorizer002 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557554</td>\n",
       "      <td>[0.4936464081959879, 0.6170023143646193]</td>\n",
       "      <td>[0.7361446008915001, 0.8026737487854864]</td>\n",
       "      <td>0.771772</td>\n",
       "      <td>0.856574</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>&lt;__main__.myVectorizer003 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.581916</td>\n",
       "      <td>[0.5233866713557235, 0.6409224857316485]</td>\n",
       "      <td>[0.7532677657041373, 0.8156159978385782]</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.863546</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>&lt;__main__.myVectorizer004 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>[0.475230730915623, 0.594753223099031]</td>\n",
       "      <td>[0.7118884689859208, 0.7774679218962482]</td>\n",
       "      <td>0.744830</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>&lt;__main__.myVectorizer005 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.551570</td>\n",
       "      <td>[0.4940827798503691, 0.6097873816891154]</td>\n",
       "      <td>[0.7325117708027957, 0.7962243345472186]</td>\n",
       "      <td>0.765205</td>\n",
       "      <td>0.851594</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>&lt;__main__.myVectorizer006 object at 0x000002A8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      F-1/2                  F-1/2 Confidence Interval  \\\n",
       "0  0.572451   [0.5135792749087205, 0.6266577060559199]   \n",
       "1  0.406654  [0.34482328016810776, 0.4676305307685182]   \n",
       "2  0.557554   [0.4936464081959879, 0.6170023143646193]   \n",
       "3  0.581916   [0.5233866713557235, 0.6409224857316485]   \n",
       "4  0.533333     [0.475230730915623, 0.594753223099031]   \n",
       "5  0.551570   [0.4940827798503691, 0.6097873816891154]   \n",
       "\n",
       "                     F1 Confidence Interval  F1 Macro  F1 Micro  Precision  \\\n",
       "0  [0.7447039328320719, 0.8069415422765013]  0.777509  0.858566   0.533333   \n",
       "1  [0.6495853047534709, 0.7205900413345668]  0.686593  0.814741   0.366667   \n",
       "2  [0.7361446008915001, 0.8026737487854864]  0.771772  0.856574   0.516667   \n",
       "3  [0.7532677657041373, 0.8156159978385782]  0.784936  0.863546   0.541667   \n",
       "4  [0.7118884689859208, 0.7774679218962482]  0.744830  0.835657   0.500000   \n",
       "5  [0.7325117708027957, 0.7962243345472186]  0.765205  0.851594   0.512500   \n",
       "\n",
       "                                          Vectorizer  \n",
       "0  <__main__.myVectorizer001 object at 0x000002A8...  \n",
       "1  <__main__.myVectorizer002 object at 0x000002A8...  \n",
       "2  <__main__.myVectorizer003 object at 0x000002A8...  \n",
       "3  <__main__.myVectorizer004 object at 0x000002A8...  \n",
       "4  <__main__.myVectorizer005 object at 0x000002A8...  \n",
       "5  <__main__.myVectorizer006 object at 0x000002A8...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#clf=svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "\n",
    "#abDF=pd.DataFrame()\n",
    "\n",
    "for vectorizer in [myVectorizer005(), myVectorizer006()]:\n",
    "\n",
    "    clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "\n",
    "    \n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 10\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, training_rows,training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    macro, micro, interval, fhalf, halfinterval=bulkEval(predictions_test, test_classes, bs=True)\n",
    "    prec=precision_score(predictions_test, test_classes)\n",
    "    \n",
    "    entry={\"Vectorizer\": vectorizer, \"F1 Macro\":macro, \"F1 Micro\":micro,\n",
    "              \"F1 Confidence Interval\":interval, \"Precision\":prec,\n",
    "          \"F-1/2\": fhalf, \"F-1/2 Confidence Interval\": halfinterval}\n",
    "    abDF=abDF.append(entry, ignore_index=True)\n",
    "    \n",
    "abDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "types= [['Word Clusters', 'TDIDF Vectors', 'Count Vectors'],\n",
    "       ['Tweet Length', 'TDIDF Vectors', 'Count Vectors'],\n",
    "       ['Tweet Length', 'Word Clusters', 'Count Vectors'],\n",
    "       ['Tweet Length', 'Word Clusters', 'TDIDF Vectors'],\n",
    "       ['Word Clusters'],\n",
    "       ['Word Clusters', 'Count Vectors']]\n",
    "\n",
    "abDF['Features Used']=types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1ci = abDF['F1 Confidence Interval']\n",
    "f1ci=[[roundToHundredths(a), roundToHundredths(b)] for [a,b] in f1ci]\n",
    "abDF['F1 Confidence Interval']=f1ci\n",
    "\n",
    "abDF['F1 Macro'] = [roundToHundredths(x) for x in abDF['F1 Macro']]\n",
    "abDF['F1 Micro'] = [roundToHundredths(x) for x in abDF['F1 Micro']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features Used</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Confidence Interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Word Clusters, TDIDF Vectors, Count Vectors]</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.86</td>\n",
       "      <td>[0.74, 0.81]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tweet Length, TDIDF Vectors, Count Vectors]</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.81</td>\n",
       "      <td>[0.65, 0.72]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Tweet Length, Word Clusters, Count Vectors]</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>[0.74, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Tweet Length, Word Clusters, TDIDF Vectors]</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.86</td>\n",
       "      <td>[0.75, 0.82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Word Clusters]</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.84</td>\n",
       "      <td>[0.71, 0.78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Word Clusters, Count Vectors]</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "      <td>[0.73, 0.8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Features Used  F1 Macro  F1 Micro  \\\n",
       "0  [Word Clusters, TDIDF Vectors, Count Vectors]      0.78      0.86   \n",
       "1   [Tweet Length, TDIDF Vectors, Count Vectors]      0.69      0.81   \n",
       "2   [Tweet Length, Word Clusters, Count Vectors]      0.77      0.86   \n",
       "3   [Tweet Length, Word Clusters, TDIDF Vectors]      0.78      0.86   \n",
       "4                                [Word Clusters]      0.74      0.84   \n",
       "5                 [Word Clusters, Count Vectors]      0.77      0.85   \n",
       "\n",
       "  F1 Confidence Interval  \n",
       "0           [0.74, 0.81]  \n",
       "1           [0.65, 0.72]  \n",
       "2            [0.74, 0.8]  \n",
       "3           [0.75, 0.82]  \n",
       "4           [0.71, 0.78]  \n",
       "5            [0.73, 0.8]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abDF[['Features Used', 'F1 Macro', 'F1 Micro', 'F1 Confidence Interval']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training size vs performance (F1 macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = svm.SVC(C=4, kernel='rbf', gamma='scale', probability=True)\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for frac in np.arange(.1, 1.01, .1):\n",
    "    \n",
    "    partial_training_set_size=int(frac*training_set_size)\n",
    "    partial_training_rows = training_rows.sample(n=partial_training_set_size)\n",
    "    partial_training_classes=partial_training_rows['Class'].tolist()\n",
    "    \n",
    "    vectorizer = myVectorizer()\n",
    "\n",
    "    #SIMPLE PIPELINE\n",
    "    pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])\n",
    "\n",
    "    grid_params = {}\n",
    "    #SEARCH HYPERPARAMETERS\n",
    "    folds = 5\n",
    "    grid = grid_search_hyperparam_space(grid_params,pipeline,folds, partial_training_rows,partial_training_classes)\n",
    "\n",
    "    #CLASSIFY AND EVALUATE \n",
    "    predictions_test = grid.predict(test_rows)\n",
    "    \n",
    "    x.append(partial_training_set_size)\n",
    "    y.append(f1(predictions_test,test_classes, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kite.com/python/answers/how-to-plot-a-linear-regression-line-on-a-scatter-plot-in-python\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.title(\"Performance vs Training Set Size\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.plot(x, [m*xi + b for xi in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The winner: Adaboost RF\n",
    "\n",
    "with countVector and clusters as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICKLE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=30, max_depth=3), n_estimators=50)\n",
    "vectorizer=myVectorizer006()\n",
    "pipeline = Pipeline(steps = [('vec',vectorizer),('classifier',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec',\n",
       "                 <__main__.myVectorizer006 object at 0x000001E0188A2FC8>),\n",
       "                ('classifier',\n",
       "                 AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=3,\n",
       "                                                                          n_estimators=30)))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(data, data['Class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\t 0.9033864541832669\n",
      "F1 Macro\t 0.8505418143144564\n",
      "F1 Micro\t 0.9033864541832669\n",
      "FHalf\t 0.6876663708961845\n",
      "Confusion Matrix\n",
      "[[0.64583333 0.35416667]\n",
      " [0.01570681 0.98429319]]\n",
      "\t****************************************\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'halfinterval' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-9b06caddb67b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpredictions_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbulkEval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-75e70d817c05>\u001b[0m in \u001b[0;36mbulkEval\u001b[1;34m(predictions_test, test_classes, bs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t****************************************\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmacro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmicro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFHalf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalfinterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#entry={\"Classifier\": clf, \"F1 Macro\":macro, \"F1 Micro\":micro,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'halfinterval' referenced before assignment"
     ]
    }
   ],
   "source": [
    "predictions_test = pipeline.predict(test_rows)\n",
    "bulkEval(predictions_test, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'adaForest11_9_2020.pickle'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just give me a function that takes an input and spits out a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'adaForest10_22_2020.pickle'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def textToPred(txt):\n",
    "    inputDF=pd.DataFrame({'Text':[txt]})\n",
    "    pred=model.predict(inputDF)[0]\n",
    "    return pred\n",
    "\n",
    "textToPred(\"My micro pen is not a self-report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioNLPEnv",
   "language": "python",
   "name": "bionlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
