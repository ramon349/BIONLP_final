{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys\\n!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\\n#'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "def loadDataAsDataFrame(f_path):\n",
    "    '''\n",
    "        Given a path, loads a data set and puts it into a dataframe\n",
    "        - simplified mechanism\n",
    "    '''\n",
    "    df = pd.read_csv(f_path, encoding='latin1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "f_path = './gary_annot.csv'\n",
    "gdata = loadDataAsDataFrame(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = './ramon_annot.csv'\n",
    "rdata = loadDataAsDataFrame(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'tweet_id', 'text', 'Annotation '], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that Ramon's header for annotations has an extra space at the end\n",
    "rdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gAnn=gdata['annotation'].tolist()\n",
    "rAnn=rdata['Annotation '].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the tweets that both people annotated\n",
    "\n",
    "commonGAnns=[]\n",
    "commonRAnns=[]\n",
    "\n",
    "for i in range(len(gAnn)):\n",
    "    ga=gAnn[i]\n",
    "    ra=rAnn[i]\n",
    "    \n",
    "    if (not np.isnan(ga)) and (not np.isnan(ra)):\n",
    "        commonGAnns.append(ga)\n",
    "        commonRAnns.append(ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48989898989898994"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score as iaaa\n",
    "\n",
    "iaaa(commonGAnns, commonRAnns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 99, 1.0: 2})\n",
      "Counter({0.0: 99, 1.0: 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(commonGAnns))\n",
    "print(Counter(commonRAnns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 1\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "1 \t 1\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "1 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n",
      "0 \t 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(commonGAnns)):\n",
    "    ga=commonGAnns[i]\n",
    "    ra=commonRAnns[i]\n",
    "    \n",
    "    print(int(ga), \"\\t\", int(ra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTwo team members manually and independently annotated the same 100 tweets to check if they were self-reports.\\nOur inter-annotator agreement was 0.49. \\nWhile startlingly low, this score is somewhat misleading.\\nIn those 100 tweets, each annotator only reported 2 self-reports, and there was a single tweet\\nthat each annotator agreed upon.\\nSuch a scarcity of data is not sufficient to draw any results from.\\nIf this study were to be continued, it would call for more manual annotation so that more\\npositive labels can be found (preferably at least 10), which would allow for a more \\nthourogh investigation into inter-annotator agreement and the limits of what \\na classifier could accomplish.\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two team members manually and independently annotated the same 100 tweets to check if they were self-reports.\n",
    "Our inter-annotator agreement was 0.49. \n",
    "While startlingly low, this score is somewhat misleading.\n",
    "In those 100 tweets, each annotator only reported 2 self-reports, and there was a single tweet\n",
    "that each annotator agreed upon.\n",
    "Such a scarcity of data is not sufficient to draw any results from.\n",
    "If this study were to be continued, it would call for more manual annotation so that more\n",
    "positive labels can be found (preferably at least 10), which would allow for a more \n",
    "thourogh investigation into inter-annotator agreement and the limits of what \n",
    "a classifier could accomplish.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioNLPEnv",
   "language": "python",
   "name": "bionlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
